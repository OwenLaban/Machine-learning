# Machine Learning Coursework & UTS ‚Äì Owen

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Jupyter Notebook](https://img.shields.io/badge/Tools-Google_Colab-orange?style=for-the-badge&logo=googlecolab&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Library-Scikit_Learn-yellow?style=for-the-badge&logo=scikit-learn&logoColor=white)
![TensorFlow](https://img.shields.io/badge/Deep_Learning-TensorFlow-ff6f00?style=for-the-badge&logo=tensorflow&logoColor=white)

## üë§ Student Identification
> This repository is submitted as part of the Machine Learning & Deep Learning coursework.

* **Name:** Josua Owen Fernandi Silaban  
* **Class:** TK-46-04  
* **NIM:** 1103223117  

---

## üìå Repository Purpose

This repository documents my learning journey in **Machine Learning** and **Deep Learning** during the course.  
It is organized to show my progress from basic data processing to building complete, end-to-end ML/DL pipelines for:

1. **Supervised Learning**
   * Regression ‚Äì predicting continuous targets (e.g. song release year).
   * Classification ‚Äì fraud detection on transaction data.
2. **Unsupervised Learning**
   * Customer clustering and representation learning using autoencoders.
3. **Model Evaluation & Interpretation**
   * Using appropriate metrics (MSE, RMSE, R¬≤, AUC, F1-score, Silhouette, etc.).
   * Visualizing model performance and feature importance.

---

## üìö Weekly Assignments

The `Weekly_Assignments` folder (or equivalent) summarizes my step-by-step learning:

- **Chapter 1‚Äì3**
  - Python refresher, NumPy arrays, Pandas DataFrame operations.
  - Basic visualizations: line plots, histograms, boxplots.

- **Chapter 4‚Äì6**
  - Data cleaning (missing values, outliers, scaling).
  - Intro to supervised learning: Linear Regression, Logistic Regression, KNN, basic model metrics.

- **Chapter 7‚Äì8**
  - Train/validation/test split, cross-validation.
  - Model tuning (GridSearch / RandomizedSearch).
  - Handling imbalanced datasets and basic feature engineering.

> Exact notebook names may differ depending on the course template used.

---

## üìÇ UTS ‚Äì End-to-End Projects

The `UTS` folder contains **three main end-to-end projects** based on the official mid-term instructions.

### 1. üïµÔ∏è‚Äç‚ôÇÔ∏è End-To-End Fraud Detection (Classification)

* **File:** `UTS_Fraud_Detection.ipynb`  
* **Dataset:** `train_transaction.csv`, `test_transaction.csv` (with `isFraud` label in train).  
* **Objective:**  
  Build a classifier that predicts the probability that a transaction is fraudulent (`isFraud = 1`), while handling **class imbalance**.

**Main Steps**

- **Data Loading & Cleaning**
  - Load train & test transaction data from Kaggle-like dataset.
  - Select a subset of informative features (e.g. `TransactionAmt`, `ProductCD`, `card4`, `card6`, `addr1`, `dist1`, etc.).
  - Handle missing values using `SimpleImputer` (median/most_frequent).

- **Preprocessing Pipeline**
  - Separate **numeric** and **categorical** features.
  - Numeric: median imputation + `StandardScaler`.
  - Categorical: most-frequent imputation + `OneHotEncoder(handle_unknown="ignore")`.
  - Combine using `ColumnTransformer`.

- **Models Explored**
  - **Logistic Regression** with `class_weight='balanced'`.
  - **Random Forest Classifier** with `class_weight='balanced'`.

- **Evaluation**
  - Train/validation split with `stratify=y`.
  - Metrics:
    - **ROC-AUC** (main metric).
    - **Classification report** (Precision, Recall, F1).
    - **Confusion Matrix** to inspect False Positive / False Negative.
    - ROC Curve plots and class distribution plot to show imbalance.

- **Prediction & Submission**
  - Apply the same preprocessing pipeline to `test_transaction.csv`.
  - Use the best model to generate `isFraud` probabilities.
  - Save as `submission_fraud.csv` with columns: `TransactionID`, `isFraud`.

---

### 2. üìà End-To-End Regression Pipeline (Song Year Prediction)

* **File:** `UTS_Regression_Pipeline.ipynb`  
* **Dataset:** `midterm-regresi-dataset.csv` (first column = target year, remaining columns = audio features).  
* **Objective:**  
  Predict the **release year** of a song based on its numerical audio features.

**Main Steps**

- **Data Understanding**
  - Load dataset without header; column 0 used as `year` target.
  - Rename features as `feature_1`, `feature_2`, ‚Ä¶ for clarity.
  - (Optional) **Row sampling** and `float32` casting to reduce RAM usage.

- **Preprocessing**
  - Clip extreme values (1%‚Äì99%) to reduce the effect of outliers.
  - Split into train/test sets.
  - Apply `StandardScaler` for all numeric features.

- **Models Explored**
  - **Linear Regression** as baseline.
  - **Random Forest Regressor** as a non-linear model.
  - (Optional) **Deep Learning Regressor** using a small Dense Neural Network in TensorFlow/Keras.

- **Evaluation Metrics**
  - **MSE** (Mean Squared Error).
  - **RMSE** (Root Mean Squared Error).
  - **MAE** (Mean Absolute Error).
  - **R¬≤ Score** (coefficient of determination).

- **Visualization**
  - Plot **True vs Predicted year** for the best model.
  - Distribution of target `year` to understand the prediction difficulty (older vs newer songs).

---

### 3. üë• Customer Clustering with Autoencoder (Unsupervised)

* **File:** `UTS_Customer_Clustering.ipynb`  
* **Dataset:** `clusteringmidterm.csv` (credit card usage & payment behavior).  
* **Objective:**  
  Segment customers into groups based on their transaction behavior, using a combination of **Autoencoder** and **K-Means Clustering**.

**Main Steps**

- **Preprocessing**
  - Drop `CUST_ID`.
  - Handle missing values (median filling).
  - Standardize all numeric features (`StandardScaler`, `float32` for memory efficiency).
  - (Optional) Row sampling for Colab RAM limitation.

- **Representation Learning (Autoencoder)**
  - Build a symmetric autoencoder with a low-dimensional latent space (e.g. 4‚Äì8 neurons).
  - Train to reconstruct input features using MSE loss.
  - Extract the **latent representation** (bottleneck layer) for each customer.

- **Clustering**
  - Run **K-Means** on latent features for various `k`.
  - Use **Elbow method** and **Silhouette Score** to select the best number of clusters.
  - Assign cluster labels back to customers.

- **Analysis**
  - Compute mean statistics per cluster (average balance, purchase frequency, cash advance, etc.).
  - Interpret business meaning of each cluster (e.g., ‚Äúhigh spender‚Äù, ‚Äúcash advance heavy user‚Äù, ‚Äúlow usage customer‚Äù).

---

## üöÄ How to Run the Notebooks

1. **Clone or Download the Repository**
   ```bash
   git clone https://github.com/owenlaban/Machine-Learning.git
   cd Machine-Learning
